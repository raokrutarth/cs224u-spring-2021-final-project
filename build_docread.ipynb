{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "build_docread.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncAB6DlYLobv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7125fc-cdd2-4d43-a772-80ca2cf4c4ec"
      },
      "source": [
        " # this mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder\n",
        "FOLDERNAME = 'CS224u/cs224u/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# this ensures that the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9TerKxqLobx"
      },
      "source": [
        "## Overview\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Fu6Px4rqUC"
      },
      "source": [
        "_author_ = \"Krutarth Rao, ANA, VM\"\n",
        "_version_ = \"CS224u, Stanford, Spring 2021\"\n",
        "\n",
        "# !pip install fuzzywuzzy[speedup]\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import utils\n",
        "import nltk\n",
        "import random\n",
        "from fuzzywuzzy import process\n",
        "from multiprocessing import Pool\n",
        "import json\n",
        "\n",
        "rand = random.SystemRandom()\n",
        "\n",
        "import rel_ext\n",
        "\n",
        "#DATA_DIR = os.path.join('data', 'rel_ext_data')\n",
        "\n",
        "DATA_DIR = os.path.join(sys.path[-1], 'data/rel_ext_data')\n",
        "\n",
        "\n",
        "KB = rel_ext.KB(DATA_DIR)\n",
        "\n",
        "entity_ids = {x.sbj for x in KB.kb_triples}\n",
        "entity_ids.union([x.obj for x in KB.kb_triples])\n",
        "\n",
        "relation_ids = {x.rel for x in KB.kb_triples}\n",
        "\n",
        "len(relation_ids), len(entity_ids)\n",
        "\n",
        "# read in docred\n",
        "files = [\"train_distant.json\", \"train_annotated.json\", \"dev.json\"]\n",
        "\n",
        "raw_relations = []\n",
        "for fn in files:\n",
        "    ds_path = os.path.join(DATA_DIR, f'doc-red/{fn}')\n",
        "    with open(ds_path, \"r\") as f:\n",
        "        raw_json = json.loads(f.read())\n",
        "        raw_relations.extend(raw_json)\n",
        "len(raw_relations)\n",
        "\n",
        "print (\"len  of raw relations: \", len (raw_relations))\n",
        "\n",
        "\n",
        "with open(os.path.join(DATA_DIR, 'doc-red/rel_info.json'), \"r\") as f:\n",
        "    relation_names = json.loads(f.read())\n",
        "\n",
        "rand.sample(list(relation_names.items()), 3)\n",
        "\n",
        "\n",
        "\n",
        "relations = set()\n",
        "for nested_rel in raw_relations:\n",
        "    for links in nested_rel[\"labels\"]:\n",
        "        head_i = links[\"h\"]\n",
        "        tail_i = links[\"t\"]\n",
        "        rel_name = relation_names[links[\"r\"]]\n",
        "\n",
        "        heads = nested_rel[\"vertexSet\"][head_i]\n",
        "        tails = nested_rel[\"vertexSet\"][tail_i]\n",
        "\n",
        "        for h in heads:\n",
        "            for t in tails:\n",
        "                relations.add(\n",
        "                    (h[\"name\"], rel_name, t[\"name\"])\n",
        "                )\n",
        "\n",
        "rand.sample(relations, 3)\n",
        "\n",
        "relations = list (relations)[:200000]\n",
        "\n",
        "print(len(relations), \"pre-id relations\")\n",
        "\n",
        "def get_id(name: str, is_relation: bool=False) -> str:\n",
        "    search_space = entity_ids\n",
        "    if is_relation:\n",
        "        search_space = relation_ids\n",
        "    match, score = process.extractOne(name, search_space)\n",
        "    if score < 85:\n",
        "        raise RuntimeError(f\"no match for {name}. Best match is {match}\")\n",
        "    return match\n",
        "\n",
        "def get_relation_dict(raw_relation):\n",
        "    sbj, rel, obj = raw_relation\n",
        "    try:\n",
        "        return dict(\n",
        "            rel=get_id(rel, is_relation=True),\n",
        "            sbj=get_id(sbj),\n",
        "            obj=get_id(obj),\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "n_cpus = 8\n",
        "chunk_size = len(relations) // n_cpus\n",
        "processed = []\n",
        "with Pool(n_cpus) as p:\n",
        "    print(\"doc-red triggered\")\n",
        "    results = p.map(get_relation_dict, relations, chunksize=chunk_size)\n",
        "    processed.extend(x for x in results if x is not None)\n",
        "\n",
        "print(len(processed), \"post-id relations\")\n",
        "\n",
        "if not len(processed):\n",
        "    print(\n",
        "        \"No relations that can be mapped to the original \"\n",
        "        \"relation/entitiy IDs found in dataset\",\n",
        "        processed\n",
        "    )\n",
        "else:\n",
        "    print(rand.sample(processed, 3))\n",
        "\n",
        "with open(\"./docred_extracted.json\", \"w+\") as f:\n",
        "    f.write(json.dumps(processed))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}